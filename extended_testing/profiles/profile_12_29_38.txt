Timer unit: 1e-09 s

Total time: 0.160068 s
Average time: 0.160068 s
File: /home/gblum/dev/DPOC_Exercise/Solver.py
Function: solution at line 31

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    31                                           def solution(C: Const) -> tuple[np.ndarray, np.ndarray]:
    32         1       2288.0   2288.0      0.0      Total_start = time.perf_counter()
    33                                           
    34         1      13357.0  13357.0      0.0      print("Computing Transition Probabilities... (fast)")
    35         1        669.0    669.0      0.0      T_start_time = time.perf_counter()
    36         1   22154275.0 2.22e+07     13.8      P_list = compute_transition_probabilities_vectorized(C)#compute_transition_probabilities_fast(C)
    37                                           
    38                                               # --- OPTIMIZATION: Pre-Stack P Matrices ---
    39         1     503621.0 503621.0      0.3      P_stack = sp.vstack(P_list).tocsr()
    40                                           
    41         1      85581.0  85581.0      0.1      del P_list
    42                                           
    43         1       2106.0   2106.0      0.0      T_end_time = time.perf_counter()
    44                                           
    45         1      19947.0  19947.0      0.0      print("Computing Stage Costs...")
    46         1        892.0    892.0      0.0      SC_start_time = time.perf_counter()
    47         1     177076.0 177076.0      0.1      Q = compute_expected_stage_cost(C)
    48         1       1713.0   1713.0      0.0      SC_end_time = time.perf_counter()
    49                                           
    50                                               # --- iGMRES & Solver Parameters ---
    51         1       5444.0   5444.0      0.0      K, L = C.K, C.L
    52         1        604.0    604.0      0.0      gamma = 1.0
    53         1       2204.0   2204.0      0.0      dtype = np.float64
    54                                           
    55         1       9077.0   9077.0      0.0      J = np.zeros(K, dtype=dtype)
    56         1       2663.0   2663.0      0.0      policy = np.zeros(K, dtype=int)
    57                                           
    58                                               # Tolerances
    59         1        801.0    801.0      0.0      gmres_tol_max = 1e-5
    60         1        761.0    761.0      0.0      gmres_tol_min = 1e-9
    61         1        220.0    220.0      0.0      eta = 0.5
    62         1        694.0    694.0      0.0      outer_tol = 1e-7
    63                                           
    64                                               # Iteration limits
    65         1        387.0    387.0      0.0      max_outer_iters = 200
    66         1        365.0    365.0      0.0      gmres_restart = 60
    67         1        396.0    396.0      0.0      max_inner_iters = 15
    68                                           
    69         1        179.0    179.0      0.0      delta_J_prev = 1.0
    70         1      11167.0  11167.0      0.0      range_k = np.arange(K, dtype=int) # Pre-compute for slicing
    71                                           
    72         1        474.0    474.0      0.0      start_time = time.perf_counter()
    73                                           
    74         1    2196814.0  2.2e+06      1.4      A_all = build_A_fast_setup(K, L, P_stack, gamma, dtype)
    75                                           
    76                                               # Begin iterations
    77        26      13792.0    530.5      0.0      for outer_iter in range(max_outer_iters):
    78        26     123580.0   4753.1      0.1          J_prev = J.copy()
    79                                           
    80                                                   # --- 1. Build A (Optimized) ---
    81                                                   # A_sparse = build_A(K, P_stack, policy, range_k, gamma, dtype)
    82        26   10952383.0 421245.5      6.8          A_sparse = build_A_fast(A_all, K, policy, range_k)
    83                                           
    84                                                   # --- 2. Preconditioner ---
    85        26    2881510.0 110827.3      1.8          M = make_preconditioner(A_sparse, omega=0.8, inner_iters=3, dtype=dtype)
    86                                           
    87                                                   # Right-hand side
    88        26    1484341.0  57090.0      0.9          b = Q[range_k, policy].astype(dtype)
    89                                           
    90                                                   # Adaptive GMRES Tolerance
    91                                                   # If delta_J is large, we don't need a perfect linear solve yet.
    92        26      12756.0    490.6      0.0          if outer_iter > 0:
    93        25      24963.0    998.5      0.0              tol_k = eta * delta_J_prev
    94                                                   else:
    95         1        237.0    237.0      0.0              tol_k = gmres_tol_max
    96                                           
    97        26      40292.0   1549.7      0.0          tol_k = min(max(tol_k, gmres_tol_min), gmres_tol_max)
    98                                           
    99                                                   # --- 3. Solve Linear System ---
   100        26       6998.0    269.2      0.0          try:
   101        52  110285937.0 2.12e+06     68.9              J_eval, info = spla.gmres(
   102        26       6211.0    238.9      0.0                  A_sparse,
   103        26       5819.0    223.8      0.0                  b,
   104        26       5984.0    230.2      0.0                  x0=J,
   105        26       5859.0    225.3      0.0                  tol=tol_k,
   106        26       6390.0    245.8      0.0                  restart=gmres_restart,
   107        26       5564.0    214.0      0.0                  maxiter=max_inner_iters,
   108        26       7463.0    287.0      0.0                  M=M
   109                                                       )
   110        26      21485.0    826.3      0.0              if info != 0:
   111                                                           J_eval = spla.spsolve(A_sparse, b)
   112                                                   except Exception:
   113                                                       J_eval = spla.spsolve(A_sparse, b)
   114                                           
   115                                                   # --- 4. Policy Improvement (Optimized) ---
   116        26    2524806.0  97107.9      1.6          P_J_all = P_stack.dot(J_eval)
   117                                           
   118        26      68569.0   2637.3      0.0          expected_future_costs = P_J_all.reshape((L, K)).T
   119                                           
   120                                                   # Bellman update
   121        26    2388339.0  91859.2      1.5          Q_J = Q + gamma * expected_future_costs
   122                                           
   123                                                   # Greedy improvement
   124        26    2189166.0  84198.7      1.4          new_policy = np.argmin(Q_J, axis=1)
   125                                           
   126                                                   # Stats
   127        26     941813.0  36223.6      0.6          policy_changes = np.sum(new_policy != policy)
   128        26     735550.0  28290.4      0.5          delta_J = np.max(np.abs(J_eval - J_prev))
   129                                           
   130        26      11700.0    450.0      0.0          delta_J_prev = delta_J
   131        26      16970.0    652.7      0.0          J = J_eval
   132        26      13688.0    526.5      0.0          policy = new_policy
   133                                           
   134        26      29646.0   1140.2      0.0          if policy_changes == 0 and delta_J < outer_tol:
   135         1      25861.0  25861.0      0.0              print(f"Converged in {outer_iter+1} iterations.")
   136         1        429.0    429.0      0.0              break
   137                                               else:
   138                                                   print("Warning: Max outer iterations reached without convergence.")
   139                                           
   140         1       1839.0   1839.0      0.0      end_time = time.perf_counter()
   141                                           
   142         1        569.0    569.0      0.0      T_TP = T_end_time - T_start_time
   143         1       7312.0   7312.0      0.0      T_SC = SC_end_time - SC_start_time
   144         1        268.0    268.0      0.0      T_J = end_time - start_time
   145         1        629.0    629.0      0.0      Total_time = time.perf_counter() - Total_start
   146                                           
   147         1       5654.0   5654.0      0.0      print("\n--- Timing Summary (Optimized with Slicing) ---")
   148         1       9710.0   9710.0      0.0      print(f"Transition Probabilities: {T_TP:.6f}s")
   149         1       2694.0   2694.0      0.0      print(f"Stage Costs:              {T_SC:.6f}s")
   150         1       2512.0   2512.0      0.0      print(f"Policy Iteration:         {T_J:.6f}s")
   151         1       4654.0   4654.0      0.0      print(f"Total Runtime:            {Total_time:.6f}s")
   152                                           
   153         1        330.0    330.0      0.0      return J, policy

Total time: 0.021788 s
Average time: 0.021788 s
File: /home/gblum/dev/DPOC_Exercise/utils.py
Function: compute_transition_probabilities_vectorized at line 258

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   258                                           def compute_transition_probabilities_vectorized(C):
   259                                               """
   260                                               Fully vectorized calculation of transition probabilities.
   261                                               Strictly adheres to PE_instructions.pdf.
   262                                           
   263                                               Corrections Implemented:
   264                                               1. Vertical Motion: y_{k+1} = y_k + v_k (Uses CURRENT velocity, not next).
   265                                               2. Pipe Logic: Shifts happen first, decrement checks NEW state.
   266                                               3. Spawn Probability: Exact linear ramp formula from PDF.
   267                                               4. m_min Logic: Defaults to M-1 (last pipe) if buffer is full.
   268                                               """
   269                                           
   270                                               # 1. Convert State Space to Matrix
   271                                               # ---------------------------------------------------------
   272         1    2911416.0 2.91e+06     13.4      S_arr = np.array(C.state_space, dtype=np.int32)
   273         1       2324.0   2324.0      0.0      N = S_arr.shape[0]
   274                                           
   275                                               # Columns: Y, V, D[0]...D[M-1], H[0]...H[M-1]
   276         1       3516.0   3516.0      0.0      Y = S_arr[:, 0]
   277         1        874.0    874.0      0.0      V = S_arr[:, 1]
   278         1       6498.0   6498.0      0.0      D = S_arr[:, 2 : 2 + C.M]
   279         1       2484.0   2484.0      0.0      H = S_arr[:, 2 + C.M : 2 + 2 * C.M]
   280                                           
   281                                               # 2. Pre-process State Lookup (SearchSorted Trick)
   282                                               # ------------------------------------------------------
   283         1      17130.0  17130.0      0.1      dtype_view = np.dtype((np.void, S_arr.dtype.itemsize * S_arr.shape[1]))
   284         1       6688.0   6688.0      0.0      S_void = np.ascontiguousarray(S_arr).view(dtype_view).ravel()
   285         1     811894.0 811894.0      3.7      sort_order = np.argsort(S_void)
   286         1      43130.0  43130.0      0.2      S_void_sorted = S_void[sort_order]
   287                                           
   288         1       1327.0   1327.0      0.0      def lookup_state_indices(next_states_matrix):
   289                                                   next_void = np.ascontiguousarray(next_states_matrix.astype(np.int32)).view(dtype_view).ravel()
   290                                                   search_indices = np.searchsorted(S_void_sorted, next_void)
   291                                                   search_indices = np.clip(search_indices, 0, N - 1)
   292                                                   found_void = S_void_sorted[search_indices]
   293                                                   valid_mask = (found_void == next_void)
   294                                                   return sort_order[search_indices], valid_mask
   295                                           
   296                                               # 3. Deterministic Pipe Dynamics (Per PDF "Dynamics" section)
   297                                               # -----------------------------------------------------------
   298                                           
   299                                               # A. Collision check
   300                                               # "On collision... transition to a cost-free termination state"
   301                                               # We identify these source states and ensure they generate NO transitions in P
   302                                               # (effectively making them absorbing or exiting the set).
   303         1       3501.0   3501.0      0.0      if C.M > 0:
   304         1       1224.0   1224.0      0.0          gap_tol = (C.G - 1) // 2
   305         1     121373.0 121373.0      0.6          is_collided = (D[:, 0] == 0) & (np.abs(Y - H[:, 0]) > gap_tol)
   306                                               else:
   307                                                   is_collided = np.zeros(N, dtype=bool)
   308                                           
   309         1      55020.0  55020.0      0.3      Hat_D = D.copy()
   310         1      68051.0  68051.0      0.3      Hat_H = H.copy()
   311                                           
   312         1       2805.0   2805.0      0.0      if C.M > 0:
   313                                                   # B. Intermediate Quantities
   314                                                   # Case 1: Passing (d[1]=0, no collision). Logic: Shift indices left.
   315         1      13809.0  13809.0      0.1          mask_passing = (D[:, 0] == 0)
   316                                           
   317         1       1527.0   1527.0      0.0          if C.M > 1:
   318                                                       # Shift indices 2..M to 1..M-1 (Python indices 1..M-1 to 0..M-2)
   319         1     100215.0 100215.0      0.5              Hat_D[mask_passing, :-1] = D[mask_passing, 1:]
   320         1      63861.0  63861.0      0.3              Hat_H[mask_passing, :-1] = H[mask_passing, 1:]
   321                                           
   322                                                   # Set last element to 0 / default height
   323         1      33225.0  33225.0      0.2          Hat_D[mask_passing, C.M-1] = 0
   324         1       3916.0   3916.0      0.0          if len(C.S_h) > 0:
   325         1      33344.0  33344.0      0.2              Hat_H[mask_passing, C.M-1] = C.S_h[0]
   326                                           
   327                                                   # C. Drift Decrement
   328                                                   # For "Normal Drift" (d[1] > 0), hat_d = d - 1.
   329                                                   # For "Passing", hat_d[1] = d[2] - 1.
   330                                                   # Since we already shifted d[2] into position 0 for passing states,
   331                                                   # we simply decrement Hat_D[0] wherever it is > 0.
   332         1      16170.0  16170.0      0.1          mask_dec = (Hat_D[:, 0] > 0)
   333         1     145972.0 145972.0      0.7          Hat_D[mask_dec, 0] -= 1
   334                                           
   335                                               # 4. Spawn Parameter Calculation
   336                                               # -----------------------------------------------
   337         1     194890.0 194890.0      0.9      sum_hat_D = np.sum(Hat_D, axis=1)
   338         1      58209.0  58209.0      0.3      s_values = (C.X - 1) - sum_hat_D
   339                                           
   340                                               # PDF Formula for p_spawn(s) implemented vectorized
   341                                               # s <= Dmin - 1: 0
   342                                               # Dmin <= s <= X - 1: linear ramp
   343                                               # s >= X: 1
   344         1      16403.0  16403.0      0.1      numerator = s_values - (C.D_min - 1)
   345         1       2805.0   2805.0      0.0      denominator = float(C.X - C.D_min)
   346         1      57373.0  57373.0      0.3      p_linear = numerator / denominator
   347         1      49684.0  49684.0      0.2      p_spawn_vec = np.clip(p_linear, 0.0, 1.0)
   348                                           
   349                                               # Find m_min: "smallest index with no assigned obstacle"
   350                                               # Python indices 1..M-1. If none, default to M-1.
   351         1      42465.0  42465.0      0.2      k_spawn_indices = np.full(N, C.M - 1, dtype=int)
   352                                           
   353         1       1274.0   1274.0      0.0      if C.M > 1:
   354         1       2430.0   2430.0      0.0          search_view = Hat_D[:, 1:]
   355         1      15234.0  15234.0      0.1          is_zero_view = (search_view == 0)
   356         1     122062.0 122062.0      0.6          first_zero_rel = np.argmax(is_zero_view, axis=1)
   357         1      47123.0  47123.0      0.2          any_zero_found = np.any(is_zero_view, axis=1)
   358         1      50035.0  50035.0      0.2          k_spawn_indices[any_zero_found] = first_zero_rel[any_zero_found] + 1
   359                                           
   360                                               # 5. Iterate Inputs & Build Matrix
   361                                               # --------------------------------
   362         1       2714.0   2714.0      0.0      prob_h_new = 1.0 / len(C.S_h) if len(C.S_h) > 0 else 0.0
   363         1       8146.0   8146.0      0.0      U_array = np.array(C.input_space)
   364                                           
   365         1       9939.0   9939.0      0.0      P_data = [[] for _ in range(C.L)]
   366         1       5194.0   5194.0      0.0      P_rows = [[] for _ in range(C.L)]
   367         1       4072.0   4072.0      0.0      P_cols = [[] for _ in range(C.L)]
   368                                           
   369         4      13363.0   3340.8      0.1      for l_idx, u in enumerate(U_array):
   370                                                   # Probabilistic Velocities
   371         3       5547.0   1849.0      0.0          if u == C.U_strong:
   372         1       3577.0   3577.0      0.0              W_flap = np.arange(-C.V_dev, C.V_dev + 1)
   373                                                   else:
   374         2       5190.0   2595.0      0.0              W_flap = np.array([0])
   375         3       3904.0   1301.3      0.0          prob_flap = 1.0 / len(W_flap)
   376         3       1224.0    408.0      0.0          n_w = len(W_flap)
   377                                           
   378                                                   # Broadcast V + u + W (Calculates v_{k+1})
   379         3     346542.0 115514.0      1.6          V_next_matrix = V[:, None] + u + W_flap[None, :] - C.g
   380         3     169141.0  56380.3      0.8          V_next_flat = np.clip(V_next_matrix, -C.V_max, C.V_max).flatten()
   381                                           
   382                                                   # Calculate y_{k+1} based on CURRENT velocity v_k (PDF Page 5 formula)
   383                                                   # y_{k+1} = min(max(y_k + v_k, 0), Y-1)
   384                                                   # We repeat Y and V (current) to match the shape of the expanded arrays
   385         3     154787.0  51595.7      0.7          Y_repeated = np.repeat(Y, n_w)
   386         3      95007.0  31669.0      0.4          V_current_repeated = np.repeat(V, n_w)
   387         3     151701.0  50567.0      0.7          Y_next_flat = np.clip(Y_repeated + V_current_repeated, 0, C.Y - 1).astype(np.int32)
   388                                           
   389                                                   # Source filtering (Collided states are dead ends)
   390         3     177681.0  59227.0      0.8          source_idxs = np.repeat(np.arange(N), n_w)
   391         3      70766.0  23588.7      0.3          valid_src = ~np.repeat(is_collided, n_w)
   392                                           
   393         3      81034.0  27011.3      0.4          V_next = V_next_flat[valid_src]
   394         3      46374.0  15458.0      0.2          Y_next = Y_next_flat[valid_src]
   395         3      87046.0  29015.3      0.4          source_idxs = source_idxs[valid_src]
   396                                           
   397                                                   # Get pre-computed pipe params for valid rows
   398         3     394024.0 131341.3      1.8          Hat_D_sub = Hat_D[source_idxs]
   399         3     444341.0 148113.7      2.0          Hat_H_sub = Hat_H[source_idxs]
   400         3     152153.0  50717.7      0.7          p_spawn_sub = p_spawn_vec[source_idxs]
   401         3     121486.0  40495.3      0.6          k_spawn_sub = k_spawn_indices[source_idxs]
   402                                           
   403                                                   # --- Path 1: No Spawn ---
   404         3     172577.0  57525.7      0.8          probs_ns = prob_flap * (1.0 - p_spawn_sub)
   405         3      26658.0   8886.0      0.1          mask_ns = probs_ns > 0
   406                                           
   407         3      85497.0  28499.0      0.4          if np.any(mask_ns):
   408         3    1867022.0 622340.7      8.6              NS_states = np.column_stack((Y_next[mask_ns], V_next[mask_ns], Hat_D_sub[mask_ns], Hat_H_sub[mask_ns]))
   409         3    3892165.0  1.3e+06     17.9              dest_idx, valid = lookup_state_indices(NS_states)
   410                                           
   411         3       2267.0    755.7      0.0              keep = valid
   412         3      77479.0  25826.3      0.4              P_rows[l_idx].append(source_idxs[mask_ns][keep])
   413         3      29030.0   9676.7      0.1              P_cols[l_idx].append(dest_idx[keep])
   414         3     101263.0  33754.3      0.5              P_data[l_idx].append(probs_ns[mask_ns][keep])
   415                                           
   416                                                   # --- Path 2: Spawn ---
   417         3      42192.0  14064.0      0.2          mask_s = (p_spawn_sub > 0)
   418                                           
   419         3      58788.0  19596.0      0.3          if np.any(mask_s) and len(C.S_h) > 0:
   420         3      22163.0   7387.7      0.1              S_Y = Y_next[mask_s]
   421         3      30345.0  10115.0      0.1              S_V = V_next[mask_s]
   422         3     137447.0  45815.7      0.6              S_D = Hat_D_sub[mask_s]
   423         3     135117.0  45039.0      0.6              S_H = Hat_H_sub[mask_s]
   424         3      48896.0  16298.7      0.2              S_k = k_spawn_sub[mask_s]
   425         3      40952.0  13650.7      0.2              S_base_prob = prob_flap * p_spawn_sub[mask_s]
   426                                           
   427                                                       # Calculate s to fill (re-computed for flattened subset)
   428         3     123469.0  41156.3      0.6              S_sum_d = np.sum(S_D, axis=1)
   429         3      46359.0  15453.0      0.2              s_fill = np.clip((C.X - 1) - S_sum_d, C.D_min, C.X - 1)
   430                                           
   431                                                       # Iterate over all possible new heights (uniform prob)
   432        12       7910.0    659.2      0.0              for h_new in C.S_h:
   433         9      28499.0   3166.6      0.1                  D_new = S_D.copy()
   434         9      19266.0   2140.7      0.1                  H_new = S_H.copy()
   435         9      37565.0   4173.9      0.2                  rows = np.arange(len(S_Y))
   436                                           
   437                                                           # Assign new pipe to the identified slot
   438         9     117362.0  13040.2      0.5                  D_new[rows, S_k] = s_fill
   439         9      79979.0   8886.6      0.4                  H_new[rows, S_k] = h_new
   440                                           
   441         9     467627.0  51958.6      2.1                  S_states = np.column_stack((S_Y, S_V, D_new, H_new))
   442         9    2388666.0 265407.3     11.0                  dest_idx, valid = lookup_state_indices(S_states)
   443                                           
   444         9      14618.0   1624.2      0.1                  keep = valid
   445         9     164433.0  18270.3      0.8                  P_rows[l_idx].append(source_idxs[mask_s][keep])
   446         9      36264.0   4029.3      0.2                  P_cols[l_idx].append(dest_idx[keep])
   447         9      79718.0   8857.6      0.4                  P_data[l_idx].append(S_base_prob[keep] * prob_h_new)
   448                                           
   449                                               # 6. Construct Sparse Matrices
   450         1        421.0    421.0      0.0      P_sparse = []
   451         4      15321.0   3830.2      0.1      for l in range(C.L):
   452         3       2257.0    752.3      0.0          if len(P_data[l]) > 0:
   453         3     225805.0  75268.3      1.0              data = np.concatenate(P_data[l])
   454         3     231251.0  77083.7      1.1              rows = np.concatenate(P_rows[l])
   455         3     218849.0  72949.7      1.0              cols = np.concatenate(P_cols[l])
   456         3    3021869.0 1.01e+06     13.9              P_l = sp.csr_matrix((data, (rows, cols)), shape=(C.K, C.K))
   457         3       4093.0   1364.3      0.0              P_sparse.append(P_l)
   458                                                   else:
   459                                                       P_sparse.append(sp.csr_matrix((C.K, C.K)))
   460                                           
   461         1        274.0    274.0      0.0      return P_sparse

