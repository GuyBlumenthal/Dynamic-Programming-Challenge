Timer unit: 1e-07 s

Total time: 0.454502 s
File: C:\Users\tobyt\OneDrive\ETH OneDrive_Personal\Master\DPOC\DPOC_Exercise\Solver.py
Function: solution at line 63

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    63                                           def solution(C: Const) -> tuple[np.ndarray, np.ndarray]:
    64         1         62.0     62.0      0.0      Total_start = time.perf_counter()
    65                                           
    66         1       1231.0   1231.0      0.0      print("Computing Transition Probabilities... (Optimized)")
    67         1         20.0     20.0      0.0      T_start_time = time.perf_counter()
    68         1    1462308.0 1.46e+06     32.2      P_list = compute_transition_probabilities_vectorized(C)
    69                                               
    70                                               # --- OPTIMIZATION: Pre-slice P matrices for fast access ---
    71                                               # Instead of vstacking and then slicing dynamically, we keep them accessible.
    72                                               # But for the cost calculation (P_stack.dot(J)), a vstack is useful.
    73         1       6127.0   6127.0      0.1      P_stack = sp.vstack(P_list).tocsr()
    74                                               
    75                                               # We also need individual CSRs for constructing A = I - gamma * P_pi efficiently
    76                                               # P_list is already [P_0, P_1, ...]. We ensure they are CSR.
    77         1         79.0     79.0      0.0      P_actions = [p.tocsr() for p in P_list]
    78                                               
    79         1         30.0     30.0      0.0      T_end_time = time.perf_counter()
    80         1         42.0     42.0      0.0      K = C.K
    81                                               
    82         1       1462.0   1462.0      0.0      print("Computing Stage Costs...")
    83         1         25.0     25.0      0.0      SC_start_time = time.perf_counter()
    84                                               # Assuming compute_expected_stage_cost_fast is already optimal (it's usually fast)
    85         1        914.0    914.0      0.0      Q = compute_expected_stage_cost_fast(C, K)
    86         1          9.0      9.0      0.0      SC_end_time = time.perf_counter()
    87                                           
    88                                               # --- iGMRES & Solver Parameters ---
    89         1        110.0    110.0      0.0      K, L = C.K, C.L
    90         1          9.0      9.0      0.0      gamma = 1.0
    91         1         21.0     21.0      0.0      dtype = np.float64
    92                                           
    93         1        218.0    218.0      0.0      J = np.zeros(K, dtype=dtype)
    94         1         44.0     44.0      0.0      policy = np.zeros(K, dtype=int)
    95                                               
    96                                               # Optimized Tolerances
    97         1          7.0      7.0      0.0      gmres_tol_max = 1e-4  # Relaxed start tolerance
    98         1          9.0      9.0      0.0      gmres_tol_min = 1e-7  # Don't over-solve inner loops (outer_tol is 1e-7)
    99         1          7.0      7.0      0.0      eta = 0.5
   100         1          8.0      8.0      0.0      outer_tol = 1e-7
   101                                               
   102         1          5.0      5.0      0.0      max_outer_iters = 200
   103         1          4.0      4.0      0.0      gmres_restart = 60
   104         1          4.0      4.0      0.0      max_inner_iters = 30  # Increased to reduce restarts
   105                                           
   106         1          4.0      4.0      0.0      delta_J_prev = 1.0
   107         1        498.0    498.0      0.0      range_k = np.arange(K, dtype=int)
   108                                               
   109         1         20.0     20.0      0.0      start_time = time.perf_counter()
   110                                               
   111                                               # Identity matrix for A construction
   112         1       3269.0   3269.0      0.1      I_mat = sp.eye(K, format='csr', dtype=dtype)
   113                                           
   114        23        171.0      7.4      0.0      for outer_iter in range(max_outer_iters):
   115        23       1795.0     78.0      0.0          J_prev = J.copy()
   116                                           
   117                                                   # --- 1. Build A (Optimized via Masking) ---
   118                                                   # A = I - gamma * sum(Mask_a * P_a)
   119                                                   # This avoids slow python indexing/slicing.
   120        23      50415.0   2192.0      1.1          P_pi = sp.csr_matrix((K, K), dtype=dtype)
   121                                                   
   122                                                   # Fast construction using diagonal masks
   123        92        973.0     10.6      0.0          for l in range(L):
   124                                                       # Create boolean mask for action l
   125        69       7237.0    104.9      0.2              mask_l = (policy == l)
   126        69      15510.0    224.8      0.3              if np.any(mask_l):
   127                                                           # Diagonal matrix of 1s where policy == l
   128        67     568674.0   8487.7     12.5                  D_l = sp.diags(mask_l.astype(int), format='csr')
   129                                                           # Add masked P matrix
   130        67     724210.0  10809.1     15.9                  P_pi += D_l.dot(P_actions[l])
   131                                                   
   132                                                   # A = I - gamma * P_pi
   133        23     158625.0   6896.7      3.5          A_sparse = I_mat - gamma * P_pi
   134                                           
   135                                                   # --- 2. Preconditioner ---
   136                                                   # Only rebuild preconditioner if needed (simple optimization: every step is fine if fast)
   137        23      33525.0   1457.6      0.7          M = make_preconditioner(A_sparse, omega=0.8, inner_iters=3, dtype=dtype)
   138                                           
   139                                                   # Right-hand side
   140        23      16197.0    704.2      0.4          b = Q[range_k, policy].astype(dtype)
   141                                           
   142                                                   # Adaptive GMRES Tolerance
   143        23        186.0      8.1      0.0          if outer_iter > 0:
   144        22        277.0     12.6      0.0              tol_k = eta * delta_J_prev
   145                                                   else:
   146         1          5.0      5.0      0.0              tol_k = gmres_tol_max
   147                                                   
   148                                                   # Clamp tolerance
   149        23        564.0     24.5      0.0          tol_k = min(max(tol_k, gmres_tol_min), gmres_tol_max)
   150                                           
   151                                                   # --- 3. Solve Linear System ---
   152        23        112.0      4.9      0.0          try:
   153                                                       # Warm start with J
   154        46    1374633.0  29883.3     30.2              J_eval, info = spla.gmres(
   155        23        124.0      5.4      0.0                  A_sparse, b, x0=J, 
   156        23        130.0      5.7      0.0                  tol=tol_k, restart=gmres_restart, 
   157        23         94.0      4.1      0.0                  maxiter=max_inner_iters, M=M
   158                                                       )
   159        23        242.0     10.5      0.0              if info != 0:
   160                                                           # Fallback only on total failure
   161                                                           J_eval = spla.spsolve(A_sparse, b)
   162                                                   except Exception:
   163                                                       J_eval = spla.spsolve(A_sparse, b)
   164                                           
   165                                                   # --- 4. Policy Improvement ---
   166                                                   # Compute costs for all actions efficiently using P_stack
   167        23      23967.0   1042.0      0.5          P_J_all = P_stack.dot(J_eval)
   168        23       1041.0     45.3      0.0          expected_future_costs = P_J_all.reshape((L, K)).T
   169                                                   
   170        23      23317.0   1013.8      0.5          Q_J = Q + gamma * expected_future_costs
   171        23      36963.0   1607.1      0.8          new_policy = np.argmin(Q_J, axis=1)
   172                                           
   173                                                   # Stats
   174        23      11739.0    510.4      0.3          policy_changes = np.sum(new_policy != policy)
   175        23      10987.0    477.7      0.2          delta_J = np.max(np.abs(J_eval - J_prev))
   176                                           
   177        23        134.0      5.8      0.0          delta_J_prev = delta_J
   178        23        271.0     11.8      0.0          J = J_eval
   179        23        197.0      8.6      0.0          policy = new_policy
   180                                           
   181        23        268.0     11.7      0.0          if policy_changes == 0 and delta_J < outer_tol:
   182         1       1672.0   1672.0      0.0              print(f"Converged in {outer_iter+1} iterations.")
   183         1         17.0     17.0      0.0              break
   184                                                   
   185        22        241.0     11.0      0.0          if outer_iter == max_outer_iters - 1:
   186                                                        print("Warning: Max outer iterations reached without convergence.")
   187                                           
   188         1         56.0     56.0      0.0      end_time = time.perf_counter()
   189                                               
   190         1         15.0     15.0      0.0      T_TP = T_end_time - T_start_time
   191         1          8.0      8.0      0.0      T_SC = SC_end_time - SC_start_time
   192         1          7.0      7.0      0.0      T_J = end_time - start_time
   193         1         15.0     15.0      0.0      Total_time = time.perf_counter() - Total_start
   194                                           
   195         1       1242.0   1242.0      0.0      print("\n--- Timing Summary (Optimized) ---")
   196         1        825.0    825.0      0.0      print(f"Transition Probabilities: {T_TP:.6f}s")
   197         1        622.0    622.0      0.0      print(f"Stage Costs:              {T_SC:.6f}s")
   198         1        573.0    573.0      0.0      print(f"Policy Iteration:         {T_J:.6f}s")
   199         1        583.0    583.0      0.0      print(f"Total Runtime:            {Total_time:.6f}s")
   200                                           
   201         1         10.0     10.0      0.0      return J, policy

Total time: 0.14513 s
File: C:\Users\tobyt\OneDrive\ETH OneDrive_Personal\Master\DPOC\DPOC_Exercise\utils.py
Function: compute_transition_probabilities_vectorized at line 202

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   202                                           def compute_transition_probabilities_vectorized(C):
   203                                               """
   204                                               Optimized transition probability calculation using Dense Coordinate Hashing 
   205                                               instead of searchsorted/views.
   206                                               """
   207                                               # 1. Parse State Space & Create Lookup Table
   208                                               # ---------------------------------------------------------
   209                                               # We assume state_space order is consistent. We need to build a map: State -> Index.
   210                                               # Instead of sorting, we use stride-based linear indexing for O(1) lookup.
   211                                               
   212                                               # Extract columns for stride calculation
   213         1      52977.0  52977.0      3.7      S_arr = np.array(C.state_space, dtype=np.int32)
   214         1         69.0     69.0      0.0      N = S_arr.shape[0]
   215                                               
   216         1         55.0     55.0      0.0      Y = S_arr[:, 0]
   217         1         22.0     22.0      0.0      V = S_arr[:, 1]
   218         1        179.0    179.0      0.0      D = S_arr[:, 2:2+C.M]
   219         1         85.0     85.0      0.0      H = S_arr[:, 2+C.M:2+2*C.M]
   220                                           
   221                                               # Determine bounds to build the dense lookup table
   222                                               # Note: We shift V to be positive for indexing
   223         1       1003.0   1003.0      0.1      v_min = V.min()
   224                                               
   225                                               # Strides calculation (assuming standard grid structure)
   226                                               # Heuristic: Max dimensions. 
   227                                               # Even if the state space is sparse, a dense table is usually small enough (MBs) and fastest.
   228         1         16.0     16.0      0.0      Y_max = C.Y
   229         1        358.0    358.0      0.0      V_range = (V.max() - v_min) + 1
   230                                               
   231                                               # Handling D and H. If M > 1, we treat D and H as flattened or packed.
   232                                               # For simplicity and speed in Flappy Bird (usually M=1 or 2), we pack D/H into single integers 
   233                                               # or just use the first pipe if that defines the state uniqueness in a structured way.
   234                                               # A robust generic hashing way:
   235                                               
   236                                               # Create a unique scalar hash for every state
   237                                               # idx = y + Y_max * (v - v_min) + Y_max*V_range * d ...
   238                                               # To avoid variable M complexity, we use np.ravel_multi_index logic manually or via unique inverse
   239                                               # But 'unique' is slow. Let's use a safe large base for hashing.
   240                                               
   241                                               # Base multipliers (Powers of 100 or similar to avoid collisions)
   242                                               # This assumes reasonable bounds: Y<1000, V<100, D<100, H<100
   243         1         12.0     12.0      0.0      base_y = 1
   244         1         15.0     15.0      0.0      base_v = 1000 # Y max is usually ~100-200
   245         1         10.0     10.0      0.0      base_d = 1000 * 100 # V range ~20
   246         1         10.0     10.0      0.0      base_h = 1000 * 100 * 100 # D range ~50
   247                                               
   248                                               # If M > 1, we only hash the first relevant pipe components or sum them with shifts
   249                                               # For strict correctness with generic M, we construct a 1D hash
   250         1       1181.0   1181.0      0.1      state_hashes = Y * base_y + (V - v_min) * base_v
   251                                               
   252                                               # Add D and H components
   253         1         14.0     14.0      0.0      current_mult = base_d
   254         3        132.0     44.0      0.0      for m in range(C.M):
   255         2        468.0    234.0      0.0          state_hashes += D[:, m] * current_mult
   256         2         31.0     15.5      0.0          current_mult *= 100 # Shift for next dim
   257         2       1892.0    946.0      0.1          state_hashes += H[:, m] * current_mult
   258         2         42.0     21.0      0.0          current_mult *= 100
   259                                           
   260                                               # Create Dense Lookup Table
   261                                               # If max hash is too large (e.g. > 10^8), this method falls back to a dictionary.
   262                                               # For Flappy Bird, it usually fits.
   263         1        236.0    236.0      0.0      max_hash = state_hashes.max()
   264                                               
   265         1         24.0     24.0      0.0      use_dense_table = (max_hash < 50_000_000) # Limit to ~200MB RAM usage for table
   266                                               
   267         1         18.0     18.0      0.0      if use_dense_table:
   268                                                   lookup_table = np.full(max_hash + 1, -1, dtype=np.int32)
   269                                                   lookup_table[state_hashes] = np.arange(N, dtype=np.int32)
   270                                                   
   271                                                   def get_indices(next_y, next_v, next_d, next_h):
   272                                                       # Compute hashes for next states
   273                                                       h_vec = next_y * base_y + (next_v - v_min) * base_v
   274                                                       
   275                                                       curr_mult = base_d
   276                                                       for m in range(C.M):
   277                                                           h_vec += next_d[:, m] * curr_mult
   278                                                           curr_mult *= 100
   279                                                           h_vec += next_h[:, m] * curr_mult
   280                                                           curr_mult *= 100
   281                                                       
   282                                                       # Filter out of bounds
   283                                                       valid_mask = (h_vec >= 0) & (h_vec <= max_hash)
   284                                                       indices = np.full(len(h_vec), -1, dtype=np.int32)
   285                                                       indices[valid_mask] = lookup_table[h_vec[valid_mask]]
   286                                                       
   287                                                       # Valid are those found in table (not -1)
   288                                                       found_mask = indices != -1
   289                                                       return indices[found_mask], found_mask
   290                                           
   291                                               else:
   292                                                   # Fallback to dict if space is huge (slower but memory safe)
   293         1      68482.0  68482.0      4.7          lookup_dict = {h: i for i, h in enumerate(state_hashes)}
   294                                                   
   295         1         29.0     29.0      0.0          def get_indices(next_y, next_v, next_d, next_h):
   296                                                        h_vec = next_y * base_y + (next_v - v_min) * base_v
   297                                                        curr_mult = base_d
   298                                                        for m in range(C.M):
   299                                                            h_vec += next_d[:, m] * curr_mult
   300                                                            curr_mult *= 100
   301                                                            h_vec += next_h[:, m] * curr_mult
   302                                                            curr_mult *= 100
   303                                                       
   304                                                        # Map using dict
   305                                                        indices = []
   306                                                        mask = []
   307                                                        for h in h_vec:
   308                                                            val = lookup_dict.get(h, -1)
   309                                                            if val != -1:
   310                                                                indices.append(val)
   311                                                                mask.append(True)
   312                                                            else:
   313                                                                mask.append(False)
   314                                                        return np.array(indices, dtype=np.int32), np.array(mask, dtype=bool)
   315                                           
   316                                               # 2. Deterministic Pipe Dynamics (Pre-computation)
   317                                               # -----------------------------------------------------------
   318                                               # (Logic identical to original, just cleaner)
   319         1         71.0     71.0      0.0      if C.M > 0:
   320         1         13.0     13.0      0.0          gap_tol = (C.G - 1) // 2
   321         1       1031.0   1031.0      0.1          is_collided = (D[:, 0] == 0) & (np.abs(Y - H[:, 0]) > gap_tol)
   322                                               else:
   323                                                   is_collided = np.zeros(N, dtype=bool)
   324                                           
   325         1        529.0    529.0      0.0      Hat_D = D.copy()
   326         1        846.0    846.0      0.1      Hat_H = H.copy()
   327                                           
   328         1         32.0     32.0      0.0      if C.M > 0:
   329         1         95.0     95.0      0.0          mask_passing = (D[:, 0] == 0)
   330         1         22.0     22.0      0.0          if C.M > 1:
   331         1        757.0    757.0      0.1              Hat_D[mask_passing, :-1] = D[mask_passing, 1:]
   332         1        400.0    400.0      0.0              Hat_H[mask_passing, :-1] = H[mask_passing, 1:]
   333                                                   
   334         1        227.0    227.0      0.0          Hat_D[mask_passing, C.M-1] = 0
   335         1         26.0     26.0      0.0          if len(C.S_h) > 0:
   336         1        216.0    216.0      0.0              Hat_H[mask_passing, C.M-1] = C.S_h[0]
   337                                           
   338         1         98.0     98.0      0.0          mask_dec = (Hat_D[:, 0] > 0)
   339         1        659.0    659.0      0.0          Hat_D[mask_dec, 0] -= 1
   340                                           
   341                                               # Spawn logic
   342         1       1296.0   1296.0      0.1      sum_hat_D = np.sum(Hat_D, axis=1)
   343         1         66.0     66.0      0.0      s_values = (C.X - 1) - sum_hat_D
   344         1        888.0    888.0      0.1      p_spawn_vec = np.clip((s_values - (C.D_min - 1)) / float(C.X - C.D_min), 0.0, 1.0)
   345                                               
   346         1        224.0    224.0      0.0      k_spawn_indices = np.full(N, C.M - 1, dtype=int)
   347         1         23.0     23.0      0.0      if C.M > 1:
   348         1         21.0     21.0      0.0          search_view = Hat_D[:, 1:]
   349         1        127.0    127.0      0.0          is_zero_view = (search_view == 0)
   350         1        347.0    347.0      0.0          any_zero_found = np.any(is_zero_view, axis=1)
   351         1        679.0    679.0      0.0          first_zero_rel = np.argmax(is_zero_view, axis=1)
   352         1        464.0    464.0      0.0          k_spawn_indices[any_zero_found] = first_zero_rel[any_zero_found] + 1
   353                                           
   354                                               # 3. Iterate Inputs & Build Matrices
   355                                               # --------------------------------
   356         1         21.0     21.0      0.0      prob_h_new = 1.0 / len(C.S_h) if len(C.S_h) > 0 else 0.0
   357         1         75.0     75.0      0.0      U_array = np.array(C.input_space)
   358                                               
   359                                               # Use lists to collect COO data
   360         1        108.0    108.0      0.0      P_rows = [[] for _ in range(C.L)]
   361         1         60.0     60.0      0.0      P_cols = [[] for _ in range(C.L)]
   362         1         53.0     53.0      0.0      P_data = [[] for _ in range(C.L)]
   363                                           
   364         4        116.0     29.0      0.0      for l_idx, u in enumerate(U_array):
   365         3         47.0     15.7      0.0          if u == C.U_strong:
   366         1         55.0     55.0      0.0              W_flap = np.arange(-C.V_dev, C.V_dev + 1)
   367                                                   else:
   368         2         55.0     27.5      0.0              W_flap = np.array([0])
   369                                                   
   370         3         54.0     18.0      0.0          prob_flap = 1.0 / len(W_flap)
   371         3         22.0      7.3      0.0          n_w = len(W_flap)
   372                                           
   373                                                   # Motion dynamics
   374                                                   # v_{k+1}
   375         3       3711.0   1237.0      0.3          V_next_matrix = V[:, None] + u + W_flap[None, :] - C.g
   376         3       1421.0    473.7      0.1          V_next_flat = np.clip(V_next_matrix, -C.V_max, C.V_max).flatten()
   377                                                   
   378                                                   # y_{k+1} (using current v)
   379         3       2974.0    991.3      0.2          Y_repeated = np.repeat(Y, n_w)
   380         3       3049.0   1016.3      0.2          V_repeated = np.repeat(V, n_w)
   381         3       1990.0    663.3      0.1          Y_next_flat = np.clip(Y_repeated + V_repeated, 0, C.Y - 1).astype(np.int32)
   382                                           
   383                                                   # Filter collided
   384         3       2947.0    982.3      0.2          source_idxs_base = np.repeat(np.arange(N), n_w)
   385         3       2414.0    804.7      0.2          valid_src = ~np.repeat(is_collided, n_w)
   386                                           
   387         3        769.0    256.3      0.1          V_next = V_next_flat[valid_src]
   388         3        413.0    137.7      0.0          Y_next = Y_next_flat[valid_src]
   389         3        408.0    136.0      0.0          source_idxs = source_idxs_base[valid_src]
   390                                                   
   391         3       5548.0   1849.3      0.4          Hat_D_sub = Hat_D[source_idxs]
   392         3       4551.0   1517.0      0.3          Hat_H_sub = Hat_H[source_idxs]
   393         3       1643.0    547.7      0.1          p_spawn_sub = p_spawn_vec[source_idxs]
   394         3        886.0    295.3      0.1          k_spawn_sub = k_spawn_indices[source_idxs]
   395                                           
   396                                                   # Path 1: No Spawn
   397         3       1049.0    349.7      0.1          probs_ns = prob_flap * (1.0 - p_spawn_sub)
   398         3        339.0    113.0      0.0          mask_ns = probs_ns > 0
   399         3        867.0    289.0      0.1          if np.any(mask_ns):
   400                                                       # O(1) Lookup
   401         6     796898.0 132816.3     54.9              dest_idx, valid = get_indices(
   402         3        235.0     78.3      0.0                  Y_next[mask_ns], 
   403         3        190.0     63.3      0.0                  V_next[mask_ns], 
   404         3       5137.0   1712.3      0.4                  Hat_D_sub[mask_ns], 
   405         3       4499.0   1499.7      0.3                  Hat_H_sub[mask_ns]
   406                                                       )
   407                                                       
   408         3        102.0     34.0      0.0              if len(dest_idx) > 0:
   409         3       1044.0    348.0      0.1                  P_rows[l_idx].append(source_idxs[mask_ns][valid])
   410         3         51.0     17.0      0.0                  P_cols[l_idx].append(dest_idx)
   411         3        882.0    294.0      0.1                  P_data[l_idx].append(probs_ns[mask_ns][valid])
   412                                           
   413                                                   # Path 2: Spawn
   414         3        763.0    254.3      0.1          mask_s = (p_spawn_sub > 0)
   415         3       1851.0    617.0      0.1          if np.any(mask_s) and len(C.S_h) > 0:
   416                                                       # Subset for spawn
   417         3        322.0    107.3      0.0              S_Y = Y_next[mask_s]
   418         3        395.0    131.7      0.0              S_V = V_next[mask_s]
   419         3       2064.0    688.0      0.1              S_D_base = Hat_D_sub[mask_s]
   420         3       1640.0    546.7      0.1              S_H_base = Hat_H_sub[mask_s]
   421         3        327.0    109.0      0.0              S_k = k_spawn_sub[mask_s]
   422         3        363.0    121.0      0.0              S_src = source_idxs[mask_s]
   423         3        691.0    230.3      0.0              S_prob = prob_flap * p_spawn_sub[mask_s] * prob_h_new
   424                                                       
   425                                                       # Calculate fill value once
   426         3       1941.0    647.0      0.1              S_sum_d = np.sum(S_D_base, axis=1)
   427         3       1047.0    349.0      0.1              s_fill = np.clip((C.X - 1) - S_sum_d, C.D_min, C.X - 1)
   428                                           
   429        12        141.0     11.8      0.0              for h_new in C.S_h:
   430                                                           # Create new pipe config
   431         9        573.0     63.7      0.0                  D_new = S_D_base.copy()
   432         9        290.0     32.2      0.0                  H_new = S_H_base.copy()
   433         9        762.0     84.7      0.1                  rows = np.arange(len(S_Y))
   434         9       2901.0    322.3      0.2                  D_new[rows, S_k] = s_fill
   435         9       2488.0    276.4      0.2                  H_new[rows, S_k] = h_new
   436                                                           
   437         9     420586.0  46731.8     29.0                  dest_idx, valid = get_indices(S_Y, S_V, D_new, H_new)
   438                                                           
   439         9        187.0     20.8      0.0                  if len(dest_idx) > 0:
   440         9        835.0     92.8      0.1                      P_rows[l_idx].append(S_src[valid])
   441         9        104.0     11.6      0.0                      P_cols[l_idx].append(dest_idx)
   442         9        576.0     64.0      0.0                      P_data[l_idx].append(S_prob[valid])
   443                                           
   444                                               # 4. Construct CSR Matrices
   445                                               # -------------------------
   446         1         10.0     10.0      0.0      P_sparse = []
   447         4        177.0     44.2      0.0      for l in range(C.L):
   448         3         30.0     10.0      0.0          if len(P_data[l]) > 0:
   449         3        909.0    303.0      0.1              data = np.concatenate(P_data[l])
   450         3        892.0    297.3      0.1              rows = np.concatenate(P_rows[l])
   451         3        605.0    201.7      0.0              cols = np.concatenate(P_cols[l])
   452                                                       # Sum duplicates automatically handled by csr_matrix constructor
   453         3      26630.0   8876.7      1.8              P_sparse.append(sp.csr_matrix((data, (rows, cols)), shape=(C.K, C.K)))
   454                                                   else:
   455                                                       P_sparse.append(sp.csr_matrix((C.K, C.K)))
   456                                           
   457         1          3.0      3.0      0.0      return P_sparse

