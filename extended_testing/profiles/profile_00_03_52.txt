Timer unit: 1e-07 s

Total time: 0.243215 s
File: C:\Users\tobyt\OneDrive\ETH OneDrive_Personal\Master\DPOC\DPOC_Exercise\Solver.py
Function: solution at line 63

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    63                                           def solution(C: Const) -> tuple[np.ndarray, np.ndarray]:
    64         1        112.0    112.0      0.0      Total_start = time.perf_counter()
    65                                           
    66         1       2064.0   2064.0      0.1      print("Computing Transition Probabilities... (fast)")
    67         1         34.0     34.0      0.0      T_start_time = time.perf_counter()
    68         1     285241.0 285241.0     11.7      P_list = compute_transition_probabilities_vectorized(C)#compute_transition_probabilities_fast(C)
    69                                           
    70                                               # --- OPTIMIZATION: Pre-Stack P Matrices ---
    71         1       5386.0   5386.0      0.2      P_stack = sp.vstack(P_list).tocsr()
    72                                           
    73         1        909.0    909.0      0.0      del P_list
    74                                           
    75         1         40.0     40.0      0.0      T_end_time = time.perf_counter()
    76         1         41.0     41.0      0.0      K = C.K
    77         1       1807.0   1807.0      0.1      print("Computing Stage Costs...")
    78         1         34.0     34.0      0.0      SC_start_time = time.perf_counter()
    79         1       1877.0   1877.0      0.1      Q = compute_expected_stage_cost_fast(C, K)
    80         1         22.0     22.0      0.0      SC_end_time = time.perf_counter()
    81                                           
    82                                               # --- iGMRES & Solver Parameters ---
    83         1        120.0    120.0      0.0      K, L = C.K, C.L
    84         1         12.0     12.0      0.0      gamma = 1.0
    85         1         15.0     15.0      0.0      dtype = np.float64
    86                                           
    87         1        398.0    398.0      0.0      J = np.zeros(K, dtype=dtype)
    88         1        185.0    185.0      0.0      policy = np.zeros(K, dtype=int)
    89                                           
    90                                               # Tolerances
    91         1          9.0      9.0      0.0      gmres_tol_max = 1e-5
    92         1          7.0      7.0      0.0      gmres_tol_min = 1e-9
    93         1          7.0      7.0      0.0      eta = 0.5
    94         1          7.0      7.0      0.0      outer_tol = 1e-7
    95                                           
    96                                               # Iteration limits
    97         1         10.0     10.0      0.0      max_outer_iters = 200
    98         1         11.0     11.0      0.0      gmres_restart = 60
    99         1          9.0      9.0      0.0      max_inner_iters = 15
   100                                           
   101         1          9.0      9.0      0.0      delta_J_prev = 1.0
   102         1        300.0    300.0      0.0      range_k = np.arange(K, dtype=int) # Pre-compute for slicing
   103                                           
   104         1         13.0     13.0      0.0      start_time = time.perf_counter()
   105                                           
   106         1      31618.0  31618.0      1.3      A_all = build_A_fast_setup(K, L, P_stack, gamma, dtype)
   107                                           
   108                                               # Begin iterations
   109        31        245.0      7.9      0.0      for outer_iter in range(max_outer_iters):
   110        31       2108.0     68.0      0.1          J_prev = J.copy()
   111                                           
   112                                                   # --- 1. Build A (Optimized) ---
   113                                                   # A_sparse = build_A(K, P_stack, policy, range_k, gamma, dtype)
   114        31     171892.0   5544.9      7.1          A_sparse = build_A_fast(A_all, K, policy, range_k)
   115                                           
   116                                                   # --- 2. Preconditioner ---
   117        31      43399.0   1400.0      1.8          M = make_preconditioner(A_sparse, omega=0.8, inner_iters=3, dtype=dtype)
   118                                           
   119                                                   # Right-hand side
   120        31      20070.0    647.4      0.8          b = Q[range_k, policy].astype(dtype)
   121                                           
   122                                                   # Adaptive GMRES Tolerance
   123                                                   # If delta_J is large, we don't need a perfect linear solve yet.
   124        31        248.0      8.0      0.0          if outer_iter > 0:
   125        30        384.0     12.8      0.0              tol_k = eta * delta_J_prev
   126                                                   else:
   127         1          6.0      6.0      0.0              tol_k = gmres_tol_max
   128                                           
   129        31        714.0     23.0      0.0          tol_k = min(max(tol_k, gmres_tol_min), gmres_tol_max)
   130                                           
   131                                                   # --- 3. Solve Linear System ---
   132        31        153.0      4.9      0.0          try:
   133        62    1717183.0  27696.5     70.6              J_eval, info = spla.gmres(
   134        31        153.0      4.9      0.0                  A_sparse,
   135        31        139.0      4.5      0.0                  b,
   136        31        136.0      4.4      0.0                  x0=J,
   137        31        132.0      4.3      0.0                  tol=tol_k,
   138        31        129.0      4.2      0.0                  restart=gmres_restart,
   139        31        135.0      4.4      0.0                  maxiter=max_inner_iters,
   140        31        123.0      4.0      0.0                  M=M
   141                                                       )
   142        31        338.0     10.9      0.0              if info != 0:
   143                                                           J_eval = spla.spsolve(A_sparse, b)
   144                                                   except Exception:
   145                                                       J_eval = spla.spsolve(A_sparse, b)
   146                                           
   147                                                   # --- 4. Policy Improvement (Optimized) ---
   148        31      35159.0   1134.2      1.4          P_J_all = P_stack.dot(J_eval)
   149                                           
   150        31       1281.0     41.3      0.1          expected_future_costs = P_J_all.reshape((L, K)).T
   151                                           
   152                                                   # Bellman update
   153        31      28550.0    921.0      1.2          Q_J = Q + gamma * expected_future_costs
   154                                           
   155                                                   # Greedy improvement
   156        31      47903.0   1545.3      2.0          new_policy = np.argmin(Q_J, axis=1)
   157                                           
   158                                                   # Stats
   159        31      14294.0    461.1      0.6          policy_changes = np.sum(new_policy != policy)
   160        31      10427.0    336.4      0.4          delta_J = np.max(np.abs(J_eval - J_prev))
   161                                           
   162        31        279.0      9.0      0.0          delta_J_prev = delta_J
   163        31        396.0     12.8      0.0          J = J_eval
   164        31        293.0      9.5      0.0          policy = new_policy
   165                                           
   166        31        437.0     14.1      0.0          if policy_changes == 0 and delta_J < outer_tol:
   167         1       1628.0   1628.0      0.1              print(f"Converged in {outer_iter+1} iterations.")
   168         1         11.0     11.0      0.0              break
   169                                               else:
   170                                                   print("Warning: Max outer iterations reached without convergence.")
   171                                           
   172         1         42.0     42.0      0.0      end_time = time.perf_counter()
   173                                           
   174         1         10.0     10.0      0.0      T_TP = T_end_time - T_start_time
   175         1          9.0      9.0      0.0      T_SC = SC_end_time - SC_start_time
   176         1          8.0      8.0      0.0      T_J = end_time - start_time
   177         1         14.0     14.0      0.0      Total_time = time.perf_counter() - Total_start
   178                                           
   179         1       1097.0   1097.0      0.0      print("\n--- Timing Summary (Optimized with Slicing) ---")
   180         1        704.0    704.0      0.0      print(f"Transition Probabilities: {T_TP:.6f}s")
   181         1        599.0    599.0      0.0      print(f"Stage Costs:              {T_SC:.6f}s")
   182         1        454.0    454.0      0.0      print(f"Policy Iteration:         {T_J:.6f}s")
   183         1        553.0    553.0      0.0      print(f"Total Runtime:            {Total_time:.6f}s")
   184                                           
   185         1         15.0     15.0      0.0      return J, policy

Total time: 0.0276812 s
File: C:\Users\tobyt\OneDrive\ETH OneDrive_Personal\Master\DPOC\DPOC_Exercise\utils.py
Function: compute_transition_probabilities_vectorized at line 202

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   202                                           def compute_transition_probabilities_vectorized(C):
   203                                               """
   204                                               Fully vectorized calculation of transition probabilities.
   205                                               Strictly adheres to PE_instructions.pdf.
   206                                           
   207                                               Corrections Implemented:
   208                                               1. Vertical Motion: y_{k+1} = y_k + v_k (Uses CURRENT velocity, not next).
   209                                               2. Pipe Logic: Shifts happen first, decrement checks NEW state.
   210                                               3. Spawn Probability: Exact linear ramp formula from PDF.
   211                                               4. m_min Logic: Defaults to M-1 (last pipe) if buffer is full.
   212                                               """
   213                                           
   214                                               # 1. Convert State Space to Matrix
   215                                               # ---------------------------------------------------------
   216         1      54558.0  54558.0     19.7      S_arr = np.array(C.state_space, dtype=np.int32)
   217         1         97.0     97.0      0.0      N = S_arr.shape[0]
   218                                           
   219                                               # Columns: Y, V, D[0]...D[M-1], H[0]...H[M-1]
   220         1         77.0     77.0      0.0      Y = S_arr[:, 0]
   221         1         24.0     24.0      0.0      V = S_arr[:, 1]
   222         1        344.0    344.0      0.1      D = S_arr[:, 2 : 2 + C.M]
   223         1         74.0     74.0      0.0      H = S_arr[:, 2 + C.M : 2 + 2 * C.M]
   224                                           
   225                                               # 2. Pre-process State Lookup (SearchSorted Trick)
   226                                               # ------------------------------------------------------
   227         1        554.0    554.0      0.2      dtype_view = np.dtype((np.void, S_arr.dtype.itemsize * S_arr.shape[1]))
   228         1        166.0    166.0      0.1      S_void = np.ascontiguousarray(S_arr).view(dtype_view).ravel()
   229         1      18560.0  18560.0      6.7      sort_order = np.argsort(S_void)
   230         1       1887.0   1887.0      0.7      S_void_sorted = S_void[sort_order]
   231                                           
   232         1         60.0     60.0      0.0      def lookup_state_indices(next_states_matrix):
   233                                                   next_void = np.ascontiguousarray(next_states_matrix.astype(np.int32)).view(dtype_view).ravel()
   234                                                   search_indices = np.searchsorted(S_void_sorted, next_void)
   235                                                   search_indices = np.clip(search_indices, 0, N - 1)
   236                                                   found_void = S_void_sorted[search_indices]
   237                                                   valid_mask = (found_void == next_void)
   238                                                   return sort_order[search_indices], valid_mask
   239                                           
   240                                               # 3. Deterministic Pipe Dynamics (Per PDF "Dynamics" section)
   241                                               # -----------------------------------------------------------
   242                                           
   243                                               # A. Collision check
   244                                               # "On collision... transition to a cost-free termination state"
   245                                               # We identify these source states and ensure they generate NO transitions in P
   246                                               # (effectively making them absorbing or exiting the set).
   247         1        115.0    115.0      0.0      if C.M > 0:
   248         1         20.0     20.0      0.0          gap_tol = (C.G - 1) // 2
   249         1       1447.0   1447.0      0.5          is_collided = (D[:, 0] == 0) & (np.abs(Y - H[:, 0]) > gap_tol)
   250                                               else:
   251                                                   is_collided = np.zeros(N, dtype=bool)
   252                                           
   253         1        794.0    794.0      0.3      Hat_D = D.copy()
   254         1       1172.0   1172.0      0.4      Hat_H = H.copy()
   255                                           
   256         1         65.0     65.0      0.0      if C.M > 0:
   257                                                   # B. Intermediate Quantities
   258                                                   # Case 1: Passing (d[1]=0, no collision). Logic: Shift indices left.
   259         1        187.0    187.0      0.1          mask_passing = (D[:, 0] == 0)
   260                                           
   261         1         46.0     46.0      0.0          if C.M > 1:
   262                                                       # Shift indices 2..M to 1..M-1 (Python indices 1..M-1 to 0..M-2)
   263         1       1072.0   1072.0      0.4              Hat_D[mask_passing, :-1] = D[mask_passing, 1:]
   264         1        890.0    890.0      0.3              Hat_H[mask_passing, :-1] = H[mask_passing, 1:]
   265                                           
   266                                                   # Set last element to 0 / default height
   267         1        378.0    378.0      0.1          Hat_D[mask_passing, C.M-1] = 0
   268         1         43.0     43.0      0.0          if len(C.S_h) > 0:
   269         1        288.0    288.0      0.1              Hat_H[mask_passing, C.M-1] = C.S_h[0]
   270                                           
   271                                                   # C. Drift Decrement
   272                                                   # For "Normal Drift" (d[1] > 0), hat_d = d - 1.
   273                                                   # For "Passing", hat_d[1] = d[2] - 1.
   274                                                   # Since we already shifted d[2] into position 0 for passing states,
   275                                                   # we simply decrement Hat_D[0] wherever it is > 0.
   276         1        124.0    124.0      0.0          mask_dec = (Hat_D[:, 0] > 0)
   277         1       1804.0   1804.0      0.7          Hat_D[mask_dec, 0] -= 1
   278                                           
   279                                               # 4. Spawn Parameter Calculation
   280                                               # -----------------------------------------------
   281         1       1516.0   1516.0      0.5      sum_hat_D = np.sum(Hat_D, axis=1)
   282         1         86.0     86.0      0.0      s_values = (C.X - 1) - sum_hat_D
   283                                           
   284                                               # PDF Formula for p_spawn(s) implemented vectorized
   285                                               # s <= Dmin - 1: 0
   286                                               # Dmin <= s <= X - 1: linear ramp
   287                                               # s >= X: 1
   288         1         63.0     63.0      0.0      numerator = s_values - (C.D_min - 1)
   289         1         50.0     50.0      0.0      denominator = float(C.X - C.D_min)
   290         1       1858.0   1858.0      0.7      p_linear = numerator / denominator
   291         1        490.0    490.0      0.2      p_spawn_vec = np.clip(p_linear, 0.0, 1.0)
   292                                           
   293                                               # Find m_min: "smallest index with no assigned obstacle"
   294                                               # Python indices 1..M-1. If none, default to M-1.
   295         1        443.0    443.0      0.2      k_spawn_indices = np.full(N, C.M - 1, dtype=int)
   296                                           
   297         1         21.0     21.0      0.0      if C.M > 1:
   298         1         24.0     24.0      0.0          search_view = Hat_D[:, 1:]
   299         1        274.0    274.0      0.1          is_zero_view = (search_view == 0)
   300         1       1146.0   1146.0      0.4          first_zero_rel = np.argmax(is_zero_view, axis=1)
   301         1        523.0    523.0      0.2          any_zero_found = np.any(is_zero_view, axis=1)
   302         1        699.0    699.0      0.3          k_spawn_indices[any_zero_found] = first_zero_rel[any_zero_found] + 1
   303                                           
   304                                               # 5. Iterate Inputs & Build Matrix
   305                                               # --------------------------------
   306         1         26.0     26.0      0.0      prob_h_new = 1.0 / len(C.S_h) if len(C.S_h) > 0 else 0.0
   307         1        102.0    102.0      0.0      U_array = np.array(C.input_space)
   308                                           
   309         1        178.0    178.0      0.1      P_data = [[] for _ in range(C.L)]
   310         1         71.0     71.0      0.0      P_rows = [[] for _ in range(C.L)]
   311         1         58.0     58.0      0.0      P_cols = [[] for _ in range(C.L)]
   312                                           
   313         4        158.0     39.5      0.1      for l_idx, u in enumerate(U_array):
   314                                                   # Probabilistic Velocities
   315         3         63.0     21.0      0.0          if u == C.U_strong:
   316         1         57.0     57.0      0.0              W_flap = np.arange(-C.V_dev, C.V_dev + 1)
   317                                                   else:
   318         2         48.0     24.0      0.0              W_flap = np.array([0])
   319         3         44.0     14.7      0.0          prob_flap = 1.0 / len(W_flap)
   320         3         25.0      8.3      0.0          n_w = len(W_flap)
   321                                           
   322                                                   # Broadcast V + u + W (Calculates v_{k+1})
   323         3       3974.0   1324.7      1.4          V_next_matrix = V[:, None] + u + W_flap[None, :] - C.g
   324         3       1612.0    537.3      0.6          V_next_flat = np.clip(V_next_matrix, -C.V_max, C.V_max).flatten()
   325                                           
   326                                                   # Calculate y_{k+1} based on CURRENT velocity v_k (PDF Page 5 formula)
   327                                                   # y_{k+1} = min(max(y_k + v_k, 0), Y-1)
   328                                                   # We repeat Y and V (current) to match the shape of the expanded arrays
   329         3       3391.0   1130.3      1.2          Y_repeated = np.repeat(Y, n_w)
   330         3       3129.0   1043.0      1.1          V_current_repeated = np.repeat(V, n_w)
   331         3       2407.0    802.3      0.9          Y_next_flat = np.clip(Y_repeated + V_current_repeated, 0, C.Y - 1).astype(np.int32)
   332                                           
   333                                                   # Source filtering (Collided states are dead ends)
   334         3       2918.0    972.7      1.1          source_idxs = np.repeat(np.arange(N), n_w)
   335         3       4173.0   1391.0      1.5          valid_src = ~np.repeat(is_collided, n_w)
   336                                           
   337         3        599.0    199.7      0.2          V_next = V_next_flat[valid_src]
   338         3        382.0    127.3      0.1          Y_next = Y_next_flat[valid_src]
   339         3        778.0    259.3      0.3          source_idxs = source_idxs[valid_src]
   340                                           
   341                                                   # Get pre-computed pipe params for valid rows
   342         3       7106.0   2368.7      2.6          Hat_D_sub = Hat_D[source_idxs]
   343         3       4362.0   1454.0      1.6          Hat_H_sub = Hat_H[source_idxs]
   344         3       1552.0    517.3      0.6          p_spawn_sub = p_spawn_vec[source_idxs]
   345         3        902.0    300.7      0.3          k_spawn_sub = k_spawn_indices[source_idxs]
   346                                           
   347                                                   # --- Path 1: No Spawn ---
   348         3       1960.0    653.3      0.7          probs_ns = prob_flap * (1.0 - p_spawn_sub)
   349         3        411.0    137.0      0.1          mask_ns = probs_ns > 0
   350                                           
   351         3       1100.0    366.7      0.4          if np.any(mask_ns):
   352         3      22591.0   7530.3      8.2              NS_states = np.column_stack((Y_next[mask_ns], V_next[mask_ns], Hat_D_sub[mask_ns], Hat_H_sub[mask_ns]))
   353         3      43711.0  14570.3     15.8              dest_idx, valid = lookup_state_indices(NS_states)
   354                                           
   355         3         32.0     10.7      0.0              keep = valid
   356         3        655.0    218.3      0.2              P_rows[l_idx].append(source_idxs[mask_ns][keep])
   357         3        305.0    101.7      0.1              P_cols[l_idx].append(dest_idx[keep])
   358         3        680.0    226.7      0.2              P_data[l_idx].append(probs_ns[mask_ns][keep])
   359                                           
   360                                                   # --- Path 2: Spawn ---
   361         3        491.0    163.7      0.2          mask_s = (p_spawn_sub > 0)
   362                                           
   363         3       1049.0    349.7      0.4          if np.any(mask_s) and len(C.S_h) > 0:
   364         3        298.0     99.3      0.1              S_Y = Y_next[mask_s]
   365         3        374.0    124.7      0.1              S_V = V_next[mask_s]
   366         3       1741.0    580.3      0.6              S_D = Hat_D_sub[mask_s]
   367         3       1527.0    509.0      0.6              S_H = Hat_H_sub[mask_s]
   368         3        263.0     87.7      0.1              S_k = k_spawn_sub[mask_s]
   369         3        492.0    164.0      0.2              S_base_prob = prob_flap * p_spawn_sub[mask_s]
   370                                           
   371                                                       # Calculate s to fill (re-computed for flattened subset)
   372         3       1444.0    481.3      0.5              S_sum_d = np.sum(S_D, axis=1)
   373         3        844.0    281.3      0.3              s_fill = np.clip((C.X - 1) - S_sum_d, C.D_min, C.X - 1)
   374                                           
   375                                                       # Iterate over all possible new heights (uniform prob)
   376        12        136.0     11.3      0.0              for h_new in C.S_h:
   377         9        475.0     52.8      0.2                  D_new = S_D.copy()
   378         9        242.0     26.9      0.1                  H_new = S_H.copy()
   379         9        569.0     63.2      0.2                  rows = np.arange(len(S_Y))
   380                                           
   381                                                           # Assign new pipe to the identified slot
   382         9       2412.0    268.0      0.9                  D_new[rows, S_k] = s_fill
   383         9       2165.0    240.6      0.8                  H_new[rows, S_k] = h_new
   384                                           
   385         9       4602.0    511.3      1.7                  S_states = np.column_stack((S_Y, S_V, D_new, H_new))
   386         9      24570.0   2730.0      8.9                  dest_idx, valid = lookup_state_indices(S_states)
   387                                           
   388         9         77.0      8.6      0.0                  keep = valid
   389         9       1023.0    113.7      0.4                  P_rows[l_idx].append(source_idxs[mask_s][keep])
   390         9        318.0     35.3      0.1                  P_cols[l_idx].append(dest_idx[keep])
   391         9       1951.0    216.8      0.7                  P_data[l_idx].append(S_base_prob[keep] * prob_h_new)
   392                                           
   393                                               # 6. Construct Sparse Matrices
   394         1          7.0      7.0      0.0      P_sparse = []
   395         4        119.0     29.8      0.0      for l in range(C.L):
   396         3         29.0      9.7      0.0          if len(P_data[l]) > 0:
   397         3        381.0    127.0      0.1              data = np.concatenate(P_data[l])
   398         3        250.0     83.3      0.1              rows = np.concatenate(P_rows[l])
   399         3        530.0    176.7      0.2              cols = np.concatenate(P_cols[l])
   400         3      26806.0   8935.3      9.7              P_l = sp.csr_matrix((data, (rows, cols)), shape=(C.K, C.K))
   401         3         25.0      8.3      0.0              P_sparse.append(P_l)
   402                                                   else:
   403                                                       P_sparse.append(sp.csr_matrix((C.K, C.K)))
   404                                           
   405         1          3.0      3.0      0.0      return P_sparse

